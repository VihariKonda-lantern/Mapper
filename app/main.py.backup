# --- main.py ---
"""
pyright: reportMissingTypeStubs=false, reportUnknownVariableType=false, reportUnknownArgumentType=false, reportUnknownMemberType=false, reportUnknownParameterType=false, reportUnknownAttributeType=false
"""
import streamlit as st  # type: ignore[import-not-found]
import pandas as pd  # type: ignore[import-not-found]
from typing import Any

st: Any = st  # type: ignore[assignment]
pd: Any = pd  # type: ignore[assignment]
import zipfile
import io
import hashlib
import os
from typing import Tuple, Optional, List, Dict, Any, Set
# Removed unused matplotlib import

# --- App Modules ---
from upload_handlers import capture_claims_file_metadata
from file_handler import (
    read_claims_with_header_option,
    detect_delimiter,
    has_header,
    is_fixed_width,
    infer_fixed_width_positions,
    parse_header_specification_file,
)
from layout_loader import (
    load_internal_layout,
    get_field_groups,
    get_required_fields,
    get_optional_fields,
    render_layout_summary_section
)
from diagnosis_loader import load_msk_bar_lookups
from mapping_engine import get_enhanced_automap
from utils import render_claims_file_summary
from transformer import transform_claims_data
from validation_engine import (
    run_validations,
    dynamic_run_validations,
)
from anonymizer import anonymize_claims_data

# --- Streamlit Setup ---
st.set_page_config(page_title="Claims Mapper and Validator", layout="wide")

# --- Custom CSS for Modern Summary Cards and UX Features ---
def inject_summary_card_css():
    """Inject CSS for modern, symmetrical summary cards and all UX enhancements."""
    st.markdown("""
    <style>
    /* Modern Page Styling */
    .main .block-container {
        padding-top: 2rem;
        padding-bottom: 2rem;
        max-width: 1400px;
    }
    
    /* Modern Headings */
    h1, h2, h3 {
        color: #1f2937 !important;
        font-weight: 600 !important;
        letter-spacing: -0.02em;
    }
    
    h2 {
        font-size: 1.75rem !important;
        margin-top: 2rem !important;
        margin-bottom: 1rem !important;
        border-bottom: 2px solid #e5e7eb;
        padding-bottom: 0.5rem;
    }
    
    h3 {
        font-size: 1.5rem !important;
        margin-top: 1.5rem !important;
        margin-bottom: 0.75rem !important;
    }
    
    /* Modern Expanders */
    .streamlit-expanderHeader {
        background-color: #f9fafb !important;
        border-radius: 8px !important;
        padding: 0.75rem 1rem !important;
        font-weight: 500 !important;
        transition: all 0.2s ease !important;
        border: 1px solid #e5e7eb !important;
    }
    
    .streamlit-expanderHeader:hover {
        background-color: #f3f4f6 !important;
        border-color: #d1d5db !important;
    }
    
    .streamlit-expanderContent {
        padding: 1rem !important;
        background-color: #ffffff !important;
        border-radius: 0 0 8px 8px !important;
        border: 1px solid #e5e7eb !important;
        border-top: none !important;
    }
    
    /* Modern Buttons - All Blue with White Text */
    .stButton > button,
    button[data-baseweb="button"],
    .stDownloadButton > button {
        background-color: #3b82f6 !important;
        color: white !important;
        border: none !important;
        border-radius: 6px !important;
        padding: 0.5rem 1rem !important;
        font-weight: 500 !important;
        transition: all 0.2s ease !important;
        box-shadow: 0 1px 2px rgba(0,0,0,0.05) !important;
    }
    
    .stButton > button:hover,
    button[data-baseweb="button"]:hover,
    .stDownloadButton > button:hover {
        background-color: #2563eb !important;
        color: white !important;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1) !important;
        transform: translateY(-1px);
    }
    
    .stButton > button:disabled,
    button[data-baseweb="button"]:disabled,
    .stDownloadButton > button:disabled {
        background-color: #9ca3af !important;
        color: white !important;
        cursor: not-allowed !important;
    }
    
    /* Form Submit Buttons */
    .stForm > div > button {
        background-color: #3b82f6 !important;
        color: white !important;
    }
    
    .stForm > div > button:hover {
        background-color: #2563eb !important;
        color: white !important;
    }
    
    /* Modern Input Fields */
    .stTextInput > div > div > input {
        border-radius: 6px !important;
        border: 1px solid #d1d5db !important;
        padding: 0.5rem 0.75rem !important;
        transition: all 0.2s ease !important;
    }
    
    .stTextInput > div > div > input:focus {
        border-color: #3b82f6 !important;
        box-shadow: 0 0 0 3px rgba(59, 130, 246, 0.1) !important;
        outline: none !important;
    }
    
    /* Modern Selectbox */
    .stSelectbox > div > div {
        border-radius: 6px !important;
        border: 1px solid #d1d5db !important;
    }
    
    /* Modern Progress Bar Container */
    div[style*="position: sticky"] {
        border-radius: 8px !important;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05) !important;
    }
    
    /* Modern Dataframes */
    .stDataFrame {
        border-radius: 8px !important;
        overflow: hidden !important;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1) !important;
    }
    
    /* Modern Forms */
    .stForm {
        border: 1px solid #e5e7eb !important;
        border-radius: 8px !important;
        padding: 1.5rem !important;
        background-color: #f9fafb !important;
    }
    
    /* Force columns to display side by side - prevent wrapping */
    .upload-columns-container {
        display: flex !important;
        flex-wrap: nowrap !important;
        width: 100% !important;
        gap: 1rem !important;
    }
    
    .upload-columns-container ~ div [data-testid="column"] {
        display: flex !important;
        flex-direction: column !important;
        flex: 1 1 0 !important;
        min-width: 0 !important;
        max-width: 100% !important;
    }
    
    /* Ensure columns display side by side */
    [data-testid="column"] {
        display: flex !important;
        flex-direction: column !important;
        flex: 1 1 0 !important;
        min-width: 0 !important;
        max-width: 100% !important;
    }
    
    /* Ensure columns have equal heights for file uploaders */
    [data-testid="column"] {
        display: flex !important;
        flex-direction: column !important;
        padding: 0.5rem !important;
        min-height: 200px !important;
    }
    
    /* Make all file uploaders the same size */
    [data-testid="column"] .stFileUploader {
        flex: 1 !important;
        min-height: 200px !important;
        height: 100% !important;
        display: flex !important;
        flex-direction: column !important;
        justify-content: center !important;
    }
    
    /* Prevent file uploaders from breaking column layout */
    .stFileUploader {
        border: 2px dashed #d1d5db !important;
        border-radius: 8px !important;
        padding: 1.5rem !important;
        background-color: #f9fafb !important;
        transition: all 0.2s ease !important;
        width: 100% !important;
        box-sizing: border-box !important;
        min-width: 0 !important;
        min-height: 200px !important;
    }
    
    .stFileUploader:hover {
        border-color: #3b82f6 !important;
        background-color: #eff6ff !important;
    }
    
    /* Ensure file uploader inner content is centered */
    .stFileUploader > div {
        display: flex !important;
        flex-direction: column !important;
        justify-content: center !important;
        align-items: center !important;
        min-height: 150px !important;
        height: 100% !important;
    }
    
    /* Hide file size limit and file type text in uploaders - multiple selectors */
    .stFileUploader small,
    .stFileUploader [data-testid="stMarkdownContainer"],
    .stFileUploader [data-testid="stMarkdownContainer"] p,
    .stFileUploader p:last-child,
    .stFileUploader div[data-baseweb="file-uploader"] + div,
    .stFileUploader div[data-baseweb="file-uploader"] ~ div {
        display: none !important;
    }
    
    /* Hide any text containing file size or type info using more aggressive selectors */
    .stFileUploader * {
        font-size: inherit;
    }
    
    /* Specifically target and hide file size/type info */
    .stFileUploader > div > div:not([data-baseweb="file-uploader"]):not([data-testid="stMarkdownContainer"]),
    .stFileUploader > div > div:last-child:not([data-baseweb="file-uploader"]) {
        display: none !important;
    }
    
    /* Ensure file uploader labels and content fit properly */
    [data-testid="column"] .stFileUploader label {
        font-size: 0.9rem !important;
        white-space: normal !important;
        word-wrap: break-word !important;
        text-align: center !important;
        margin-bottom: 0.5rem !important;
    }
    
    /* Ensure file uploader containers don't overflow */
    [data-testid="column"] > div {
        width: 100% !important;
        min-width: 0 !important;
        overflow: hidden !important;
        flex: 1 !important;
        display: flex !important;
        flex-direction: column !important;
    }
    
    /* Make file uploader drag area consistent */
    .stFileUploader [data-baseweb="file-uploader"] {
        width: 100% !important;
        min-height: 150px !important;
        display: flex !important;
        align-items: center !important;
        justify-content: center !important;
    }
    
    /* Modern Alerts/Info Boxes */
    .stAlert {
        border-radius: 8px !important;
        border-left: 4px solid !important;
        padding: 1rem !important;
    }
    
    /* Style summary cards wrapper - target all columns */
    .summary-cards-wrapper [data-testid="column"] {
        background: white;
        border-radius: 12px;
        padding: 1.5rem !important;
        box-shadow: 0 2px 8px rgba(0,0,0,0.08);
        border: 1px solid #e8e8e8;
        min-height: 450px;
        display: flex;
        flex-direction: column;
        transition: all 0.3s ease;
    }
    
    /* Ensure equal heights using flexbox */
    .summary-cards-wrapper [data-testid="column"] > div {{
        display: flex;
        flex-direction: column;
        flex: 1;
    }}
    
    /* Style headings within cards */
    .summary-cards-wrapper h4 {
        margin-top: 0 !important;
        margin-bottom: 1rem !important;
        font-size: 16px !important;
        font-weight: 600 !important;
        color: #1a1a1a !important;
    }
    
    /* Consistent spacing for text in cards */
    .summary-cards-wrapper .stMarkdown p {
        margin-bottom: 0.5rem !important;
        line-height: 1.6 !important;
        color: #1a1a1a !important;
    }
    
    /* Reduce spacing in Layout and Lookup summaries to match Claims File Summary */
    .summary-cards-wrapper [data-testid="column"]:nth-child(2) .stMarkdown,
    .summary-cards-wrapper [data-testid="column"]:nth-child(3) .stMarkdown {
        margin-bottom: 0.75rem !important;
        margin-top: 0.1rem !important;
    }
    
    .summary-cards-wrapper [data-testid="column"]:nth-child(2) .stMarkdown p,
    .summary-cards-wrapper [data-testid="column"]:nth-child(3) .stMarkdown p {
        margin-bottom: 0.75rem !important;
        line-height: 1.2 !important;
    }
    
    /* Style expanders in cards - ensure consistent spacing */
    .summary-cards-wrapper .streamlit-expanderHeader {
        font-size: 14px !important;
        font-weight: 500 !important;
        margin-top: 0.5rem !important;
        color: #1a1a1a !important;
    }
    
    .summary-cards-wrapper .streamlit-expander {{
        margin-top: 0.25rem !important;
    }}
    
    /* Add bottom margin to last expander to balance spacing */
    .summary-cards-wrapper [data-testid="column"] .streamlit-expander:last-child {{
        margin-bottom: 0.5rem !important;
    }}
    
    /* Style info boxes */
    .summary-cards-wrapper .stAlert {{
        margin-top: 1rem !important;
    }}
    
    /* Compact spacing for file description line */
    .summary-cards-wrapper [data-testid="column"] .stMarkdown:first-of-type p {{
        margin-bottom: 0.5rem !important;
    }}
    
    /* Reduce spacing between title and first content */
    .summary-cards-wrapper h4 + .stMarkdown {{
        margin-top: -0.5rem !important;
    }}
    
    /* Tooltip Styles */
    .tooltip-wrapper {{
        position: relative;
        display: inline-block;
        cursor: help;
    }}
    
    .tooltip-text {{
        visibility: hidden;
        width: 200px;
        background-color: #333;
        color: #fff;
        text-align: center;
        border-radius: 6px;
        padding: 8px;
        position: absolute;
        z-index: 1000;
        bottom: 125%;
        left: 50%;
        margin-left: -100px;
        opacity: 0;
        transition: opacity 0.3s;
        font-size: 12px;
    }}
    
    .tooltip-wrapper:hover .tooltip-text {{
        visibility: visible;
        opacity: 1;
    }}
    
    /* Visual Indicators */
    .status-indicator {{
        display: inline-block;
        width: 12px;
        height: 12px;
        border-radius: 50%;
        margin-right: 6px;
        animation: pulse 2s infinite;
    }}
    
    .status-mapped {{ background-color: #4caf50; }}
    .status-unmapped {{ background-color: #f44336; }}
    .status-suggested {{ background-color: #ff9800; }}
    
    @keyframes pulse {{
        0%, 100% {{ opacity: 1; }}
        50% {{ opacity: 0.5; }}
    }}
    
    /* Progress Indicators */
    .progress-container {{
        position: relative;
        width: 100%;
        height: 20px;
        background-color: #e0e0e0;
        border-radius: 10px;
        overflow: hidden;
        margin: 10px 0;
    }}
    
    .progress-bar {{
        height: 100%;
        border-radius: 10px;
        transition: width 0.5s ease;
    }}
    
    /* Drag and Drop Styles */
    .drag-drop-zone {{
        border: 2px dashed #4caf50;
        border-radius: 8px;
        padding: 40px;
        text-align: center;
        background-color: rgba(76, 175, 80, 0.05);
        transition: all 0.3s ease;
    }}
    
    .drag-drop-zone.dragover {{
        border-color: #2e7d32;
        background-color: rgba(76, 175, 80, 0.1);
    }}
    
    /* Search/Filter Styles */
    .search-box {
        padding: 10px;
        border: 1px solid #e8e8e8;
        border-radius: 6px;
        width: 100%;
        margin-bottom: 10px;
        background-color: white;
        color: #1a1a1a;
    }
    
    /* Keyboard Shortcut Hints */
    .shortcut-hint {{
        font-size: 11px;
        color: #888;
        font-style: italic;
        margin-left: 8px;
    }}
    
    /* Auto-save Indicator */
    .autosave-indicator {{
        position: fixed;
        bottom: 20px;
        right: 20px;
        background-color: #4caf50;
        color: white;
        padding: 8px 16px;
        border-radius: 20px;
        font-size: 12px;
        z-index: 1000;
        opacity: 0;
        transition: opacity 0.3s;
    }}
    
    .autosave-indicator.show {{
        opacity: 1;
    }}
    
    /* Modern Tabs */
    .stTabs [data-baseweb="tab-list"] {
        gap: 0.5rem;
        background-color: #f9fafb;
        padding: 0.5rem;
        border-radius: 8px;
    }
    
    .stTabs [data-baseweb="tab"] {
        border-radius: 6px;
        padding: 0.75rem 1.5rem;
        transition: all 0.2s ease;
    }
    
    .stTabs [aria-selected="true"] {
        background-color: white;
        color: #3b82f6;
        font-weight: 600;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
    
    /* Modern Divider */
    hr {
        border: none;
        border-top: 1px solid #e5e7eb;
        margin: 2rem 0;
    }
    
    /* Smooth Transitions */
    * {
        transition: background-color 0.2s ease, color 0.2s ease, border-color 0.2s ease;
    }
    
    /* Modern Code Blocks */
    .stCodeBlock {
        border-radius: 6px !important;
        border: 1px solid #e5e7eb !important;
    }
    
    /* Spacing Improvements */
    .element-container {
        margin-bottom: 1rem;
    }
    
    /* Modern Container for Tools */
    .streamlit-expanderHeader[aria-expanded="true"] {
        background-color: #eff6ff !important;
        border-color: #3b82f6 !important;
    }
    </style>
    """, unsafe_allow_html=True)

# --- Helpers ---
def _notify(msg: str) -> None:
    st.toast(msg)

# --- UX Feature Helpers ---
def inject_ux_javascript():
    """Inject JavaScript for UX enhancements and hide file uploader size/type text."""
    st.markdown("""
    <script>
    (function() {
        // Keyboard Shortcuts
        document.addEventListener('keydown', function(e) {
            // Tab navigation (Ctrl/Cmd + Arrow keys)
            if ((e.ctrlKey || e.metaKey) && e.key === 'ArrowRight') {
                e.preventDefault();
                const tabs = document.querySelectorAll('[data-baseweb="tab"]');
                const activeTab = document.querySelector('[data-baseweb="tab"][aria-selected="true"]');
                if (activeTab && tabs.length > 0) {
                    const currentIndex = Array.from(tabs).indexOf(activeTab);
                    if (currentIndex < tabs.length - 1) {
                        tabs[currentIndex + 1].click();
                    }
                }
            }
            if ((e.ctrlKey || e.metaKey) && e.key === 'ArrowLeft') {
                e.preventDefault();
                const tabs = document.querySelectorAll('[data-baseweb="tab"]');
                const activeTab = document.querySelector('[data-baseweb="tab"][aria-selected="true"]');
                if (activeTab && tabs.length > 0) {
                    const currentIndex = Array.from(tabs).indexOf(activeTab);
                    if (currentIndex > 0) {
                        tabs[currentIndex - 1].click();
                    }
                }
            }
            // Search (Ctrl/Cmd + F)
            if ((e.ctrlKey || e.metaKey) && e.key === 'f') {
                e.preventDefault();
                const searchInput = document.querySelector('input[placeholder*="Search"]');
                if (searchInput) searchInput.focus();
            }
        });
        
        // Hide file size limit and file type text in uploaders
        function hideFileUploaderText() {
            document.querySelectorAll('.stFileUploader').forEach(function(uploader) {
                // Hide small tags (file size/type info)
                uploader.querySelectorAll('small').forEach(function(el) {
                    el.style.display = 'none';
                });
                // Hide markdown containers with file size/type info
                uploader.querySelectorAll('[data-testid="stMarkdownContainer"]').forEach(function(el) {
                    const text = el.textContent || '';
                    if (text.includes('Limit') || text.includes('MB') || text.includes('XLSX') || 
                        text.includes('CSV') || text.includes('TXT') || text.includes('TSV') ||
                        text.includes('JSON') || text.includes('PARQUET')) {
                        el.style.display = 'none';
                    }
                });
                // Hide any paragraph that contains file size/type info
                uploader.querySelectorAll('p').forEach(function(el) {
                    const text = el.textContent || '';
                    if (text.includes('Limit') || text.includes('MB') || text.includes('XLSX') || 
                        text.includes('CSV') || text.includes('TXT') || text.includes('TSV') ||
                        text.includes('JSON') || text.includes('PARQUET')) {
                        el.style.display = 'none';
                    }
                });
            });
        }
        
        // Run immediately and on mutations
        hideFileUploaderText();
        setTimeout(hideFileUploaderText, 500);
        setTimeout(hideFileUploaderText, 1000);
        
        // Watch for new elements being added
        const observer = new MutationObserver(function(mutations) {
            hideFileUploaderText();
        });
        observer.observe(document.body, { childList: true, subtree: true });
        
        // Drag and Drop Enhancement
        setTimeout(function() {
            document.querySelectorAll('input[type="file"]').forEach(function(input) {
                const parent = input.closest('.stFileUploader');
                if (parent) {
                    parent.addEventListener('dragover', function(e) {
                        e.preventDefault();
                        parent.style.border = '2px dashed #4caf50';
                        parent.style.backgroundColor = 'rgba(76, 175, 80, 0.1)';
                    });
                    parent.addEventListener('dragleave', function(e) {
                        e.preventDefault();
                        parent.style.border = '';
                        parent.style.backgroundColor = '';
                    });
                    parent.addEventListener('drop', function(e) {
                        e.preventDefault();
                        parent.style.border = '';
                        parent.style.backgroundColor = '';
                    });
                }
            });
        }, 500);
    })();
    </script>
    """, unsafe_allow_html=True)

def render_tooltip(text: str, help_text: str) -> str:
    """Generate HTML for tooltip."""
    return f'<span class="tooltip-wrapper" title="{help_text}">{text}</span>'

def render_status_indicator(status: str) -> str:
    """Generate HTML for status indicator."""
    status_class = {
        "mapped": "status-mapped",
        "unmapped": "status-unmapped",
        "suggested": "status-suggested"
    }.get(status, "status-unmapped")
    return f'<span class="status-indicator {status_class}"></span>'

def render_progress_bar(percent: int, label: str = "") -> str:
    """Generate HTML for animated progress bar."""
    return f'<div class="progress-container" style="margin-top: 0.75rem; margin-bottom: 0.5rem; background-color: rgba(255,255,255,0.2); border-radius: 4px; height: 8px; overflow: hidden;"><div style="width: {percent}%; background: transparent !important; height: 100%; border-radius: 4px; transition: width 0.5s ease;"></div></div><small style="color: rgba(255,255,255,0.9); font-size: 0.9rem; display: block; margin-top: 0.5rem;">{label}</small>'

def save_mapping_template(final_mapping: Dict[str, Dict[str, Any]], filename: str = "mapping_template.json") -> str:
    """Export mapping as JSON template."""
    import json
    return json.dumps(final_mapping, indent=2)

def load_mapping_template(template_json: str) -> Dict[str, Dict[str, Any]]:
    """Load mapping from JSON template."""
    import json
    return json.loads(template_json)

def initialize_undo_redo():
    """Initialize undo/redo history for mappings."""
    if "mapping_history" not in st.session_state:
        st.session_state.mapping_history = []
    if "history_index" not in st.session_state:
        st.session_state.history_index = -1

def save_to_history(final_mapping: Dict[str, Dict[str, Any]]):
    """Save current mapping state to history."""
    initialize_undo_redo()
    # Remove any future history if we're not at the end
    if st.session_state.history_index < len(st.session_state.mapping_history) - 1:
        st.session_state.mapping_history = st.session_state.mapping_history[:st.session_state.history_index + 1]
    # Add new state
    import copy
    st.session_state.mapping_history.append(copy.deepcopy(final_mapping))
    st.session_state.history_index = len(st.session_state.mapping_history) - 1
    # Limit history size
    if len(st.session_state.mapping_history) > 50:
        st.session_state.mapping_history.pop(0)
        st.session_state.history_index -= 1

def undo_mapping() -> Optional[Dict[str, Dict[str, Any]]]:
    """Undo last mapping change."""
    initialize_undo_redo()
    if st.session_state.history_index > 0:
        st.session_state.history_index -= 1
        return st.session_state.mapping_history[st.session_state.history_index]
    return None

def redo_mapping() -> Optional[Dict[str, Dict[str, Any]]]:
    """Redo last undone mapping change."""
    initialize_undo_redo()
    if st.session_state.history_index < len(st.session_state.mapping_history) - 1:
        st.session_state.history_index += 1
        return st.session_state.mapping_history[st.session_state.history_index]
    return None

# --- Utility Functions ---
def custom_file_uploader(label_text: str, key: str, types: List[str]) -> Optional[Any]:
    """Custom wrapper for st.file_uploader with a clean label and minimal visual noise."""
    st.markdown(f"<div style='margin-bottom: 4px; font-weight: 500;'>{label_text}</div>", unsafe_allow_html=True)
    return st.file_uploader(label="", type=types, key=key, label_visibility="collapsed")

def render_file_upload_button(label: str, key: str, types: List[str]) -> Optional[Any]:
    """Custom button-driven file uploader."""
    container = st.empty()

    if f"{key}_clicked" not in st.session_state:
        st.session_state[f"{key}_clicked"] = False

    if not st.session_state[f"{key}_clicked"]:
        if container.button(f"üìÅ {label}", key=f"btn_{key}"):
            st.session_state[f"{key}_clicked"] = True

    if st.session_state[f"{key}_clicked"]:
        return container.file_uploader("", type=types, key=key, label_visibility="collapsed")
    
    return None

def render_title():
    """Renders the app title."""
    st.markdown("<div style='font-size: 22px; font-weight: 600; margin-bottom: 10px;'>Claims File Mapper and Validator</div>", unsafe_allow_html=True)

@st.cache_data(show_spinner=False)
def load_layout_cached(file: Any) -> Any:
    return load_internal_layout(file)

@st.cache_data(show_spinner=False)
def load_lookups_cached(file: Any):
    return load_msk_bar_lookups(file)

def render_lookup_summary_section():
    """Preview summary for diagnosis lookup codes.

    Dynamically reads all code sets from session state and displays counts.
    Automatically detects all code categories (MSK, BAR, and any future categories).
    """
    # Dynamically find all code categories in session_state (anything ending with "_codes")
    code_categories = {}
    for key in st.session_state.keys():
        if key.endswith("_codes") and isinstance(st.session_state[key], (set, list)):
            category_name = key.replace("_codes", "").upper()
            code_categories[category_name] = st.session_state[key]
    
    if code_categories:
        st.markdown("#### Diagnosis Lookup Summary")
        
        # Display total code categories count
        total_categories = len(code_categories)
        st.markdown(f"**Total Code Categories:** {total_categories}", unsafe_allow_html=True)
        
        # Display counts for each category dynamically
        for category_name, codes in sorted(code_categories.items()):
            st.markdown(f"**{category_name} Codes Loaded:** {len(codes)}", unsafe_allow_html=True)

        # Create expanders for each category dynamically
        for category_name, codes in sorted(code_categories.items()):
            with st.expander(f"View Sample {category_name} Codes"):
                code_list = list(codes) if isinstance(codes, set) else codes
                st.write(code_list[:10])  # type: ignore[no-untyped-call]
    elif st.session_state.get("lookup_upload_attempted", False):
        # Only show info if user has attempted to upload a file
        st.info("Upload a valid diagnosis lookup file to preview codes.")

@st.cache_data(show_spinner=False)
def generate_mapping_table(layout_df: Any, final_mapping: Dict[str, Dict[str, Any]], claims_df: Any) -> Any:
    """Build a comprehensive mapping table across internal and claims fields.

    Includes internal fields with mapped claims columns, data types, and
    required/optional flag, plus a section for unmapped claims columns.

    Args:
        layout_df: Internal layout DataFrame-like.
        final_mapping: Final field mappings `{internal: {"value": col}}`.
        claims_df: Uploaded claims DataFrame-like.

    Returns:
        DataFrame-like representing the full mapping table.
    """
    records: List[Dict[str, Any]] = []

    # --- 1. Internal Layout Side ---
    for _, row in layout_df.iterrows():
        internal_field = row["Internal Field"]
        usage = row["Usage"].strip().lower()
        required_status = "Required" if usage == "mandatory" else "Optional"

        mapped_column = final_mapping.get(internal_field, {}).get("value", "")
        data_type = ""

        if mapped_column and mapped_column in claims_df.columns:
            data_type = str(claims_df[mapped_column].dtype)  # type: ignore[no-untyped-call]

        records.append({
            "Internal Field": internal_field,
            "Claims File Column": mapped_column,
            "Data Type": data_type,
            "Required/Optional": required_status
        })

    # --- 2. Unmapped Claims Columns Side ---
    mapped_claims_cols: Set[str] = {str(info.get("value", "")) for info in final_mapping.values()}
    all_claims_cols = set(claims_df.columns)

    unmapped_claims_cols = all_claims_cols - mapped_claims_cols

    for col in sorted(unmapped_claims_cols):
        data_type = str(claims_df[col].dtype)  # type: ignore[no-untyped-call]
        records.append({
            "Internal Field": "",
            "Claims File Column": col,
            "Data Type": data_type,
            "Required/Optional": "Unmapped Claim Column"
        })

    # --- Final Mapping Table ---
    mapping_table = pd.DataFrame(records)
    return mapping_table

def render_upload_and_claims_preview():
    """
    Upload Section for Layout, Lookup, Claims, and Headerless detection.
    Allows proper external header upload if needed.
    """
    st.markdown("### Step 1: Upload Files and Confirm Claims File Headers")

    # Check if header file uploader should be shown (only when headerless file detected)
    show_header_uploader = False
    if "detected_has_header" in st.session_state:
        # Show uploader if file was detected as headerless
        if st.session_state.detected_has_header is False:
            show_header_uploader = True
        # Also show if user already uploaded a header file (to allow changes)
        if st.session_state.get("header_file_obj") is not None:
            show_header_uploader = True
    
    # --- All Upload Blocks Side by Side ---
    # Always create columns based on whether header uploader is needed
    if show_header_uploader:
        # 4 columns: Layout, Lookup, Header, Claims - equal widths with gap
        col1, col2, col3, col4 = st.columns([1, 1, 1, 1], gap="large")
    else:
        # 3 columns: Layout, Lookup, Claims - equal widths with gap
        col1, col2, col3 = st.columns([1, 1, 1], gap="large")
    
    # Initialize variables
    layout_file = None
    lookup_file = None
    header_file = None
    claims_file = None
    
    # Layout File Upload (always in col1)
    with col1:
        layout_file = st.file_uploader("üìÑ Upload CDM Layout File", type=["xlsx"], key="layout_file", help="Excel file (.xlsx) containing internal field definitions and requirements. Drag and drop or click to upload.")
        if layout_file:
            st.session_state.layout_upload_attempted = True
            layout_df = load_layout_cached(layout_file)
            st.session_state.layout_df = layout_df
            # Track upload order if this is a new file
            if "layout_file_obj" not in st.session_state or st.session_state.layout_file_obj.name != layout_file.name:
                if "upload_order" not in st.session_state:
                    st.session_state.upload_order = []
                if "layout" not in st.session_state.upload_order:
                    st.session_state.upload_order.append("layout")
            st.session_state.layout_file_obj = layout_file
        elif layout_file is None and "layout_upload_attempted" not in st.session_state:
            # Track that user has interacted with uploader (even if no file selected)
            pass
        elif layout_file is None and "layout" in st.session_state.get("upload_order", []):
            # File was removed, remove from order
            if "upload_order" in st.session_state:
                st.session_state.upload_order.remove("layout")

    # Lookup File Upload (always in col2)
    with col2:
        lookup_file = st.file_uploader("üìã Upload Lookup File", type=["xlsx"], key="lookup_file", help="Excel file (.xlsx) containing MSK and BAR diagnosis codes. Drag and drop or click to upload.")
        if lookup_file:
            st.session_state.lookup_upload_attempted = True
            msk_codes, bar_codes = load_lookups_cached(lookup_file)
            st.session_state.msk_codes = msk_codes
            st.session_state.bar_codes = bar_codes
            # Track upload order if this is a new file
            if "lookup_file_obj" not in st.session_state or st.session_state.lookup_file_obj.name != lookup_file.name:
                if "upload_order" not in st.session_state:
                    st.session_state.upload_order = []
                if "lookup" not in st.session_state.upload_order:
                    st.session_state.upload_order.append("lookup")
            st.session_state.lookup_file_obj = lookup_file
        elif lookup_file is None and "lookup_upload_attempted" not in st.session_state:
            # Track that user has interacted with uploader (even if no file selected)
            pass
        elif lookup_file is None and "lookup" in st.session_state.get("upload_order", []):
            # File was removed, remove from order
            if "upload_order" in st.session_state:
                st.session_state.upload_order.remove("lookup")

    # Header File Upload (only in col3 when needed)
    if show_header_uploader:
        with col3:
            header_file = st.file_uploader(
                "üìù Upload Header File (Required)", 
                type=["csv", "txt", "tsv", "xlsx", "xls"], 
                key="external_header_upload", 
                help="Required for files without headers. Supports: (1) Simple format: one column (vertical) or one row (horizontal) with column names only. (2) Specification format: file with Column Name, Start Position, End Position, and Size columns (for fixed-width files)."
            )
            st.session_state.header_file_obj = header_file if header_file else None
        # Claims File Upload (in col4 when header is shown)
        with col4:
            claims_file = st.file_uploader("üìä Upload Claims File", 
                                          type=["csv", "txt", "tsv", "xlsx", "xls", "json", "parquet"],
                                          key="claims_file_upload",
                                          help="Main claims data file (.csv, .txt, .tsv, .xlsx, .xls, .json, .parquet) to be mapped and validated. Drag and drop or click to upload.")
    else:
        # Claims File Upload (in col3 when header is not shown)
        with col3:
            claims_file = st.file_uploader("üìä Upload Claims File", 
                                          type=["csv", "txt", "tsv", "xlsx", "xls", "json", "parquet"],
                                          key="claims_file_upload",
                                          help="Main claims data file (.csv, .txt, .tsv, .xlsx, .xls, .json, .parquet) to be mapped and validated. Drag and drop or click to upload.")
    
    # Handle claims file upload (common logic for both cases)
    if claims_file:
        st.session_state.claims_upload_attempted = True
        # Reset session if file changed
        if "claims_file_obj" in st.session_state and st.session_state.claims_file_obj.name != claims_file.name:
            st.session_state.pop("claims_df", None)
            st.session_state.pop("final_mapping", None)
            st.session_state.pop("auto_mapping", None)
            st.session_state.pop("auto_mapped_fields", None)
        # Track upload order if this is a new file
        if "claims_file_obj" not in st.session_state or st.session_state.claims_file_obj.name != claims_file.name:
            if "upload_order" not in st.session_state:
                st.session_state.upload_order = []
            if "claims" not in st.session_state.upload_order:
                st.session_state.upload_order.append("claims")
            st.session_state.claims_file_obj = claims_file
    elif claims_file is None and "claims_upload_attempted" not in st.session_state:
        # Track that user has interacted with uploader (even if no file selected)
        pass
    elif claims_file is None and "claims" in st.session_state.get("upload_order", []):
        # File was removed, remove from order
        if "upload_order" in st.session_state:
            st.session_state.upload_order.remove("claims")

    # --- Automatic File Loading ---
    # Check if all files are uploaded and claims_df hasn't been loaded yet
    all_files_uploaded = (
        "layout_file_obj" in st.session_state and 
        "lookup_file_obj" in st.session_state and 
        "claims_file_obj" in st.session_state
    )
    
    # Check if we need to load (either not loaded yet, or file changed)
    claims_file = st.session_state.get("claims_file_obj")
    last_loaded = st.session_state.get("last_loaded_file")
    needs_loading = (
        all_files_uploaded and 
        claims_file is not None and
        (st.session_state.get("claims_df") is None or 
         last_loaded != claims_file.name)
    )
    
    if needs_loading:
        try:
            # --- Intelligent File Format Detection ---
            claims_file.seek(0)
            ext = claims_file.name.lower()
            
            # Detect if file is fixed-width
            is_fw = False
            colspecs = None
            delimiter = None
            
            if ext.endswith((".csv", ".txt", ".tsv")):
                claims_file.seek(0)
                file_content = claims_file.read()
                claims_file.seek(0)
                file_bytes = io.BytesIO(file_content)
                
                # Check if fixed-width
                is_fw = is_fixed_width(file_bytes)
                
                if is_fw:
                    st.info("üìè **Detected:** Fixed-width file format")
                    # Check if header file is a specification file first (before auto-detection)
                    header_file = st.session_state.get("header_file_obj")
                    
                    if header_file:
                        # Try to parse as header specification file
                        try:
                            header_file.seek(0)
                            header_bytes = header_file.read()
                            header_file.seek(0)
                            header_ext = os.path.splitext(header_file.name)[-1].lower()
                            header_spec_names, header_spec_colspecs = parse_header_specification_file(header_bytes, header_ext)
                            if header_spec_colspecs:
                                st.success(f"‚úÖ **Header Specification File Detected:** Found {len(header_spec_names)} columns with position information")
                                colspecs = header_spec_colspecs
                                st.info("üìè Using column positions from header specification file")
                        except (ValueError, Exception):
                            # Not a header spec file, will try auto-detection
                            pass
                    
                    # If no header spec file or parsing failed, try auto-detection
                    if not colspecs:
                        colspecs = infer_fixed_width_positions(file_bytes)
                        if colspecs:
                            st.success(f"‚úÖ Auto-detected {len(colspecs)} columns")
                            # Store in session state for potential manual override
                            st.session_state.fixed_width_colspecs = colspecs
                        else:
                            st.warning("‚ö†Ô∏è Could not auto-detect column positions. Please upload a header specification file or manual specification may be required.")
                else:
                    # Delimited file
                    delimiter = detect_delimiter(claims_file)
            
            # Check if header file is a specification file (for delimited files - use names only)
            header_file = st.session_state.get("header_file_obj")
            if header_file and not is_fw and ext.endswith((".csv", ".txt", ".tsv")):
                # For delimited files, check if header file is a spec file (unlikely but possible)
                try:
                    header_file.seek(0)
                    header_bytes = header_file.read()
                    header_file.seek(0)
                    header_ext = os.path.splitext(header_file.name)[-1].lower()
                    temp_names, temp_colspecs = parse_header_specification_file(header_bytes, header_ext)
                    if temp_colspecs:
                        st.info(f"‚ÑπÔ∏è Header specification file detected (contains {len(temp_names)} columns), but file is delimited. Using column names only.")
                        # For delimited files, we only use the names, not positions
                        header_spec_names = temp_names
                        header_spec_colspecs = None
                except (ValueError, Exception):
                    # Not a header spec file, will be handled as regular header file
                    pass
            
            # Auto-detect if file has headers (for CSV/TXT/TSV files)
            # Always detect, but require header file if no headers found
            detected_has_header = None
            
            # Auto-detect headers (always, to inform user)
            if ext.endswith((".csv", ".txt", ".tsv")):
                if not is_fw:
                    # For delimited text files, use intelligent detection
                    claims_file.seek(0)
                    file_content = claims_file.read()
                    claims_file.seek(0)
                    file_bytes = io.BytesIO(file_content)
                    detected_has_header = has_header(file_bytes, delimiter=delimiter or ",")
                    st.session_state.detected_has_header = detected_has_header
                else:
                    # For fixed-width, check first row
                    claims_file.seek(0)
                    file_content = claims_file.read()
                    claims_file.seek(0)
                    file_bytes = io.BytesIO(file_content)
                    detected_has_header = has_header(file_bytes, delimiter="")  # No delimiter for fixed-width
                    st.session_state.detected_has_header = detected_has_header
            elif ext.endswith((".xlsx", ".xls")):
                # Excel files typically always have headers
                detected_has_header = True
                st.session_state.detected_has_header = True
            else:
                # For other formats, assume headers exist
                detected_has_header = True
                st.session_state.detected_has_header = True
            
            # --- Preview first 3 rows
            with st.spinner("Preparing preview..."):
                claims_file.seek(0)
                if is_fw and colspecs:
                    # Preview fixed-width file
                    try:
                        file_content = claims_file.read()
                        claims_file.seek(0)
                        from file_handler import detect_encoding
                        encoding = detect_encoding(file_content)
                        file_like = io.BytesIO(file_content)
                        preview_df = pd.read_fwf(file_like, colspecs=colspecs, nrows=3, header=None, encoding=encoding, dtype=str)  # type: ignore[no-untyped-call]
                    except Exception as e:
                        st.warning(f"Could not preview fixed-width file: {e}")
                        # Fallback: show raw lines
                        claims_file.seek(0)
                        file_content = claims_file.read()
                        claims_file.seek(0)
                        from file_handler import detect_encoding
                        encoding = detect_encoding(file_content)
                        lines = []
                        for i, line_bytes in enumerate(file_content.split(b'\n')):
                            if i >= 3:
                                break
                            try:
                                line_text = line_bytes.decode(encoding or 'utf-8', errors='ignore')[:100]
                                lines.append({"Line": i+1, "Content": line_text})
                            except:
                                pass
                        preview_df = pd.DataFrame(lines)  # type: ignore[no-untyped-call]
                elif ext.endswith((".csv", ".txt", ".tsv")):
                    preview_df = pd.read_csv(claims_file, nrows=3, header=None, delimiter=delimiter, on_bad_lines="skip")  # type: ignore[no-untyped-call]
                else:
                    preview_df = pd.read_excel(claims_file, nrows=3, header=None)  # type: ignore[no-untyped-call]

                st.markdown("**Preview of Claims File (First 3 Rows):**")
                st.dataframe(preview_df, use_container_width=True)  # type: ignore[no-untyped-call]
                
                # Show fixed-width column positions if detected
                if is_fw and colspecs:
                    with st.expander("üìè View Detected Column Positions"):
                        st.markdown("**Column Positions (start, end):**")
                        for i, (start, end) in enumerate(colspecs):
                            st.write(f"Column {i+1}: Positions {start}-{end} (width: {end-start})")
                
                # Show header detection result
                if detected_has_header is not None:
                    if detected_has_header:
                        st.info("‚úÖ **Auto-detected:** File appears to have headers. Using first row as column names.")
                    else:
                        st.error("‚ùå **Auto-detected:** File appears to have no headers. **Header file is required.**")
                        st.markdown("""
                        **Please upload a header file in one of these formats:**
                        - **One row format:** All field names in a single row (e.g., `claim_id,patient_name,date_of_service`)
                        - **One column format:** All field names in a single column (one per row)
                        """)
                        # Check if header file is provided
                        if not header_file:
                            st.warning("‚ö†Ô∏è **Please upload a header file to continue.**")
                            # Trigger rerun to show header uploader
                            if "header_uploader_shown" not in st.session_state:
                                st.session_state.header_uploader_shown = True
                                st.rerun()
                            st.stop()  # Stop processing until header file is uploaded

            # --- Read full claims file
            with st.spinner("Loading claims file..."):
                claims_file.seek(0)
                header_file = st.session_state.get("header_file_obj")
                # Use header file if provided (allows override), otherwise use detected status
                # If no headers detected and no header file, we should have stopped earlier
                use_header_file = header_file is not None
                # Determine if we should use header spec file or regular header file
                use_header_spec = header_spec_colspecs is not None and header_spec_names is not None
                
                claims_df = read_claims_with_header_option(
                    claims_file,
                    headerless=(use_header_file or (detected_has_header is False) or use_header_spec),
                    header_file=header_file if (use_header_file and not use_header_spec) else None,
                    delimiter=delimiter,
                    colspecs=colspecs if is_fw else None,
                    header_names=header_spec_names if use_header_spec else None
                )

                # Fallback for junk columns
                if claims_df.columns.isnull().any():
                    claims_df.columns = [f"col_{i}" if not col or pd.isna(col) else str(col) for i, col in enumerate(claims_df.columns)]

                # Save to session
                st.session_state.claims_df = claims_df
                st.session_state.last_loaded_file = claims_file.name
                # Use detected header status if available, otherwise fallback to logic
                final_has_header = detected_has_header if detected_has_header is not None else (header_file is None)
                capture_claims_file_metadata(claims_file, has_header=final_has_header)

                st.success("‚úÖ Claims file loaded successfully using " +
                           ("embedded headers." if header_file is None else "external header file."))
                st.rerun()  # Rerun to update UI

        except Exception as e:
            st.error(f"Error processing claims file: {e}")
    # Don't show the message - files will load automatically when all are uploaded
    # The upload areas themselves are clear enough

def dual_input_field(field_label: str, options: List[str], key_prefix: str) -> Optional[str]:
    """
    Renders a dropdown + manual text input side-by-side.
    Returns whichever the user picked or typed.
    """
    col1, col2 = st.columns([2, 3])

    with col1:
        dropdown_selection = st.selectbox(
            f"Select {field_label}", 
            options=[""] + options,
            key=f"{key_prefix}_dropdown"
        )

    with col2:
        manual_entry = st.text_input(
            f"Or manually enter {field_label}", 
            key=f"{key_prefix}_manual"
        )

    if manual_entry.strip():
        return manual_entry.strip()
    else:
        return dropdown_selection.strip() if dropdown_selection else None
    
def render_field_mapping_tab():
    """
    Renders the Field Mapping tab where users map source columns to internal fields.
    Fields are grouped by category and separated by required/optional.
    """
    layout_df = st.session_state.get("layout_df")
    claims_df = st.session_state.get("claims_df")

    if layout_df is None or claims_df is None:
        st.info("Please upload both layout and claims files to begin mapping.")
        return

    # Initialize undo/redo
    initialize_undo_redo()

    # Initialize widget counter for unique keys (reset each time function is called)
    widget_counter = 0

    # --- Claims File Preview with Real-time Updates ---
    st.markdown("#### Claims File Preview (First 5 Rows)")
    preview_df = claims_df.head()
    
    # Apply search filter if active
    search_query = st.session_state.get("field_search", "")
    if search_query and search_query.strip():
        preview_df = preview_df.filter(regex=search_query, axis=1)  # type: ignore[no-untyped-call]
    
    st.dataframe(preview_df, use_container_width=True)  # type: ignore[no-untyped-call]
    
    # Real-time preview of mapped data
    if st.session_state.get("final_mapping"):
        with st.expander("üîç Real-time Mapping Preview", expanded=False):
            try:
                preview_mapped = transform_claims_data(claims_df.head(10), st.session_state.final_mapping)
                st.dataframe(preview_mapped, use_container_width=True)  # type: ignore[no-untyped-call]
            except Exception as e:
                st.warning(f"Preview unavailable: {e}")

    required_fields = get_required_fields(layout_df)

    # --- Safety Check ---
    if not isinstance(required_fields, pd.DataFrame):  # type: ignore[redundant-cast,type-py]
        st.error("Error: Required fields extraction failed. Please check layout file format.")
        st.stop()

    field_groups = get_field_groups(required_fields)

    if "final_mapping" not in st.session_state:
        st.session_state.final_mapping = {}

    final_mapping: Dict[str, Dict[str, Any]] = st.session_state.get("final_mapping", {})

    # --- AI Auto-Mapping Suggestions ---
    if "auto_mapping" not in st.session_state or not st.session_state.auto_mapping:
        st.session_state.auto_mapping = get_enhanced_automap(layout_df, claims_df)

    ai_suggestions = st.session_state.get("auto_mapping", {})

    # --- Auto-Apply High Confidence Suggestions (‚â•80%) ---
    if "final_mapping" not in st.session_state:
        st.session_state.final_mapping = {}

    auto_mapped: List[str] = []
    for field, info in ai_suggestions.items():
        score = info.get("score", 0)
        if score >= 80 and field not in final_mapping:
            final_mapping[field] = {
                "mode": "auto",
                "value": info["value"]
            }
            auto_mapped.append(field)

    st.session_state.auto_mapped_fields = auto_mapped

    # --- Mapping Progress ---
    _mapped_required, total_required, _percent_complete = calculate_mapping_progress(layout_df, final_mapping)
    
    # --- Required Fields Heading ---
    st.markdown("<br>", unsafe_allow_html=True)
    st.markdown("### Mandatory Fields Mapping")
            
    # --- Required Fields Mapping ---
    # Get search query from session state
    search_query = st.session_state.get("field_search", "")
    
    for group in field_groups:
        group_fields = required_fields[required_fields["Category"] == group]
        
        # Deduplicate fields within the group - keep first occurrence of each field
        group_fields = group_fields.drop_duplicates(subset=["Internal Field"], keep="first")  # type: ignore[no-untyped-call]

        # Apply search filter
        if search_query and search_query.strip():
            search_lower = search_query.lower()
            group_fields = group_fields[
                group_fields["Internal Field"].str.lower().str.contains(search_lower, na=False)  # type: ignore[no-untyped-call]
            ]

        group_field_names = group_fields["Internal Field"].tolist()
        mapped_count = sum(
            1 for f in group_field_names
            if f in final_mapping and final_mapping[f].get("value")
        )
        total_in_group = len(group_field_names)
        
        group_label = f"{group} ({mapped_count}/{total_in_group} mapped)"

        with st.expander(group_label, expanded=False):
            for row_idx, (original_idx, row) in enumerate(group_fields.iterrows()):
                field_name = row["Internal Field"]
                
                # Show AI suggestion if available
                has_suggestion = field_name in ai_suggestions
                if has_suggestion:
                    suggestion_score = ai_suggestions[field_name].get("score", 0)
                    st.markdown(f"**{field_name}** <span style='color: #ff9800; font-size: 0.85em;'>(AI: {suggestion_score}%)</span>", unsafe_allow_html=True)
                else:
                    st.markdown(f"**{field_name}**")
                raw_columns = claims_df.columns.tolist()

                # --- AI suggestion if available ---
                suggestion_info = ai_suggestions.get(field_name, {})
                suggested_column = suggestion_info.get("value")
                suggestion_score = suggestion_info.get("score")

                col_options: List[str] = []
                for col in raw_columns:
                    if col == suggested_column:
                        label = f"{col} (AI Suggested)"
                        if suggestion_score:
                            label = f"{col} (AI: {suggestion_score}%)"
                        col_options.append(str(label))
                    else:
                        col_options.append(str(col))

                default_value = final_mapping.get(field_name, {}).get("value", None)
                default_label: Optional[str] = None
                if default_value:
                    if default_value == suggested_column and suggestion_score:
                        default_label = f"{default_value} (AI: {suggestion_score}%)"
                    else:
                        default_label = str(default_value)

                # Create unique key using group, field name, and incrementing counter
                # Counter ensures absolute uniqueness even with duplicate rows
                widget_counter += 1
                unique_key = f"req_{group}_{field_name}_{widget_counter}"
                # Sanitize key to remove special characters that might cause issues
                unique_key = unique_key.replace(" ", "_").replace("/", "_").replace("\\", "_").replace("-", "_").replace(".", "_")

                selected_clean: Optional[str] = None
                if field_name in ["Plan_Sponsor_Name", "Insurance_Plan_Name", "Client_Name"]:
                    widget_counter += 1
                    key_prefix = f"req_{group}_{field_name}_{widget_counter}"
                    key_prefix = key_prefix.replace(" ", "_").replace("/", "_").replace("\\", "_").replace("-", "_").replace(".", "_")
                    selected = dual_input_field(field_name, col_options, key_prefix=key_prefix)
                    manual_key = f"{key_prefix}_manual"
                    manual_value = st.session_state.get(manual_key)

                    if manual_value:
                        selected_clean = manual_value.strip()
                    else:
                        selected_clean = selected.replace(f" (AI: {suggestion_score}%)", "").replace(" (AI Suggested)", "") if selected else None
                else:
                    select_idx = col_options.index(default_label) + 1 if (default_label is not None and default_label in col_options) else 0
                    selected = st.selectbox(
                        "Select column:",
                        options=[""] + col_options,
                        index=select_idx,
                        key=unique_key,
                        help=f"Map {field_name} to a claims file column"
                    )
                    selected_clean = selected.replace(f" (AI: {suggestion_score}%)", "").replace(" (AI Suggested)", "") if selected else None

                if selected_clean:
                    old_mapping = final_mapping.get(field_name)
                    new_mapping = {
                            "mode": "manual" if selected_clean != suggested_column else "auto",
                            "value": selected_clean
                        }
                    final_mapping[field_name] = new_mapping
                    # Don't save to history here - wait for form submission

    # --- Show Unmapped Required Fields ---
    unmapped = [
        field for field in required_fields["Internal Field"].tolist()
        if field not in final_mapping or not final_mapping[field]["value"]
    ]

    if total_required == 0:
        st.info("No required fields defined in layout file.")
    elif unmapped:
        st.markdown(
            f"""
            <div style="border: 1px solid #ffa94d; background-color: #ffffff; padding: 16px; border-radius: 8px; margin-top: 10px;">
                <strong style="color: #d9480f;">‚ö†Ô∏è Unmapped Required Fields</strong><br>
                <span style="color: #212529;">{', '.join(unmapped)}</span>
            </div>
            """,
            unsafe_allow_html=True
        )
    else:
        st.success("All required fields mapped.")

    # --- Optional Fields Mapping ---
    st.markdown("<br>", unsafe_allow_html=True)
    st.markdown("### Optional Fields Mapping")

    optional_fields = get_optional_fields(layout_df)

    # --- Safety Check ---
    if not isinstance(optional_fields, pd.DataFrame):  # type: ignore[redundant-cast,type-py]
        st.error("Error: Optional fields extraction failed. Please check layout file format.")
        st.stop()

    if "Internal Field" not in optional_fields.columns or "Category" not in optional_fields.columns:
        st.error("Error: Layout file must contain 'Internal Field' and 'Category' columns.")
        st.stop()

    optional_fields = optional_fields[optional_fields["Internal Field"].notnull()]
    optional_fields = optional_fields[optional_fields["Category"].notnull()]
    optional_fields = optional_fields[~optional_fields["Internal Field"].isin(required_fields["Internal Field"].tolist())]  # type: ignore[no-untyped-call]

    # --- Optional Fields Mapping ---
    if not optional_fields.empty:
        optional_field_groups = get_field_groups(optional_fields)

        for group in optional_field_groups:
            group_fields = optional_fields[optional_fields["Category"] == group]
            
            # Deduplicate fields within the group - keep first occurrence of each field
            group_fields = group_fields.drop_duplicates(subset=["Internal Field"], keep="first")  # type: ignore[no-untyped-call]

            group_field_names = group_fields["Internal Field"].tolist()
            mapped_count = sum(
                1 for f in group_field_names
                if f in final_mapping and final_mapping[f].get("value")
            )
            total_in_group = len(group_field_names)
            group_label = f"{group} ({mapped_count}/{total_in_group} mapped)"

            with st.expander(group_label, expanded=False):
                for row_idx, (original_idx, row) in enumerate(group_fields.iterrows()):
                    field_name = row["Internal Field"]
                    raw_columns = claims_df.columns.tolist()

                    suggestion_info = ai_suggestions.get(field_name, {})
                    suggested_column = suggestion_info.get("value")
                    suggestion_score = suggestion_info.get("score")

                    col_options: List[str] = []
                    for col in raw_columns:
                        if col == suggested_column:
                            label = f"{col} (AI Suggested)"
                            if suggestion_score:
                                label = f"{col} (AI: {suggestion_score}%)"
                            col_options.append(str(label))
                        else:
                            col_options.append(str(col))

                    default_value = final_mapping.get(field_name, {}).get("value", None)
                    default_label = None
                    if default_value:
                        if default_value == suggested_column and suggestion_score:
                            default_label = f"{default_value} (AI: {suggestion_score}%)"
                        else:
                            default_label = default_value

                    # Create unique key using group, field name, and incrementing counter
                    # Counter ensures absolute uniqueness even with duplicate rows
                    widget_counter += 1
                    unique_key = f"opt_{group}_{field_name}_{widget_counter}"
                    # Sanitize key to remove special characters that might cause issues
                    unique_key = unique_key.replace(" ", "_").replace("/", "_").replace("\\", "_").replace("-", "_").replace(".", "_")

                    selected_clean: Optional[str] = None
                    if field_name in ["Plan_Sponsor_Name", "Insurance_Plan_Name", "Client_Name"]:
                        widget_counter += 1
                        key_prefix = f"opt_{group}_{field_name}_{widget_counter}"
                        key_prefix = key_prefix.replace(" ", "_").replace("/", "_").replace("\\", "_").replace("-", "_").replace(".", "_")
                        selected = dual_input_field(field_name, col_options, key_prefix=key_prefix)
                    else:
                        selected = st.selectbox(
                            f"{field_name}",
                            options=[""] + col_options,
                            index=col_options.index(default_label) + 1 if default_label in col_options else 0,
                            key=unique_key
                        )
                        selected_clean = selected.replace(f" (AI: {suggestion_score}%)", "").replace(" (AI Suggested)", "") if selected else None

                    if selected_clean:
                        final_mapping[field_name] = {
                            "mode": "manual" if selected_clean != suggested_column else "auto",
                            "value": selected_clean
                        }

    # --- After ALL mappings, refresh transformed_df ---
    claims_df = st.session_state.get("claims_df")
    final_mapping = st.session_state.get("final_mapping", {})

    if claims_df is not None and final_mapping:
        st.session_state.transformed_df = transform_claims_data(claims_df, final_mapping)
        st.session_state.final_mapping = final_mapping

    if st.session_state.get("final_mapping"):
        generate_all_outputs()

def calculate_mapping_progress(layout_df: Any, final_mapping: Dict[str, Dict[str, Any]]) -> Tuple[int, int, int]:
    """Calculates progress stats for required fields mapping."""
    required_fields = get_required_fields(layout_df)["Internal Field"].tolist()
    mapped_fields = [field for field in required_fields if field in final_mapping and final_mapping[field]["value"]]
    total_required = len(required_fields)
    mapped_required = len(mapped_fields)
    percent_complete = int((mapped_required / total_required) * 100) if total_required > 0 else 0

    return mapped_required, total_required, percent_complete

def generate_all_outputs():
    """
    Generates anonymized claims file and mapping table outputs
    based on the latest field mappings.
    """
    final_mapping = st.session_state.get("final_mapping")
    claims_df = st.session_state.get("claims_df")
    layout_df = st.session_state.get("layout_df")

    if final_mapping and claims_df is not None and layout_df is not None:
        # --- Extract basic filename (placeholders removed) ---

        # --- Generate outputs ---
        anonymized_df = anonymize_claims_data(claims_df, final_mapping)
        mapping_table = generate_mapping_table(layout_df, final_mapping, claims_df)  # <-- Fixed this line

        # --- Save in session_state
        st.session_state.anonymized_df = anonymized_df
        st.session_state.mapping_table = mapping_table
    else:
        st.session_state.anonymized_df = None
        st.session_state.mapping_table = None

def render_claims_file_deep_dive():
    """
    Displays the deep dive summary of the claims file after mapping:
    showing all internal fields, mapped columns, data types, fill rates, required flag, and pass/fail status.
    """
    st.markdown("#### Claims File Deep Dive (Mapped Columns Analysis)")

    claims_df = st.session_state.get("claims_df")
    final_mapping = st.session_state.get("final_mapping", {})
    layout_df = st.session_state.get("layout_df")
    transformed_df = st.session_state.get("transformed_df")

    if claims_df is None or final_mapping is None or layout_df is None or transformed_df is None:
        st.info("Upload claims file, complete mappings, and preview transformed data first.")
        return

    deep_dive_rows: List[Dict[str, Any]] = []

    internal_fields = layout_df["Internal Field"].tolist()

    for internal_field in internal_fields:
        source_column = final_mapping.get(internal_field, {}).get("value", None)

        if source_column and source_column in transformed_df.columns:
            col_dtype = str(transformed_df[source_column].dtype)
            fill_rate = 100 * transformed_df[source_column].notnull().sum() / len(transformed_df)

            if fill_rate == 0:
                status_icon = "‚ùå Fail"
            elif fill_rate < 95:
                status_icon = "‚ö†Ô∏è Warning"
            else:
                status_icon = "‚úÖ Pass"
        else:
            col_dtype = "N/A"
            fill_rate = 0.0
            status_icon = "‚ùå Fail"

        # Check if field is required
        usage = layout_df[layout_df["Internal Field"] == internal_field]["Usage"].values
        is_required = usage[0].lower() == "required" if len(usage) > 0 else False

        deep_dive_rows.append({
            "Internal Field": internal_field,
            "Mapped Claims Column": source_column if source_column else "Not Mapped",
            "Data Type": col_dtype,
            "Fill Rate %": f"{fill_rate:.1f}",
            "Required": "Yes" if is_required else "No",
            "Status": status_icon
        })

    deep_dive_df = pd.DataFrame(deep_dive_rows)

    st.dataframe(deep_dive_df, use_container_width=True)  # type: ignore[no-untyped-call]

    deep_dive_csv = deep_dive_df.to_csv(index=False).encode('utf-8')
    st.download_button(
        label="üì• Download Deep Dive CSV",
        data=deep_dive_csv,
        file_name="deep_dive_report.csv",
        mime="text/csv",
        key="download_deep_dive_csv"
    )

# --- App Layout ---
# Inject UX JavaScript and CSS
inject_ux_javascript()
render_title()
tab1, tab2, tab3, tab4 = st.tabs(["Setup", "Field Mapping", "Preview & Validate", "Downloads Tab"])

with tab1:
    render_upload_and_claims_preview()
    
    # Inject CSS for modern cards
    inject_summary_card_css()
    
    # Dynamic summary cards based on upload order
    upload_order = st.session_state.get("upload_order", [])
    
    # Build list of summaries to render based on upload order
    summary_functions = []
    summary_map = {
        "layout": render_layout_summary_section,
        "lookup": render_lookup_summary_section,
        "claims": render_claims_file_summary
    }
    
    # Add summaries in upload order
    for file_type in upload_order:
        if file_type in summary_map:
            # Verify file is still uploaded
            if file_type == "layout" and "layout_file_obj" in st.session_state:
                summary_functions.append(summary_map[file_type])
            elif file_type == "lookup" and "lookup_file_obj" in st.session_state:
                summary_functions.append(summary_map[file_type])
            elif file_type == "claims" and "claims_file_obj" in st.session_state:
                summary_functions.append(summary_map[file_type])
    
    # Render summaries dynamically
    if summary_functions:
        st.markdown('<div class="summary-cards-wrapper">', unsafe_allow_html=True)
        num_summaries = len(summary_functions)
        
        # Create columns based on number of summaries (1, 2, or 3)
        if num_summaries == 1:
            cols = st.columns(1, gap="large")
        elif num_summaries == 2:
            cols = st.columns(2, gap="large")
        else:
            cols = st.columns(3, gap="large")
        
        # Render each summary in its column
        for i, summary_func in enumerate(summary_functions):
            with cols[i]:
                summary_func()
        
        st.markdown('</div>', unsafe_allow_html=True)

with tab2:
    layout_df = st.session_state.get("layout_df")
    claims_df = st.session_state.get("claims_df")
    final_mapping = st.session_state.get("final_mapping", {})

    if layout_df is None or claims_df is None:
        st.info("Upload both layout and claims files to begin mapping.")
        st.stop()

    # --- Sticky Mapping Progress Bar ---
    # Guard layout_df and string operations
    required_fields = layout_df[layout_df["Usage"].astype(str).str.lower() == "required"]["Internal Field"].tolist()  # type: ignore[no-untyped-call]
    total_required = len(required_fields) if required_fields else 0
    mapped_required = [f for f in required_fields if f in final_mapping and final_mapping[f].get("value")]
    mapped_count = len(mapped_required)
    percent_complete = int((mapped_count / total_required) * 100) if total_required > 0 else 0

    progress_html = render_progress_bar(percent_complete, f"{mapped_count} / {total_required} fields mapped ({percent_complete}%)")
    st.markdown(
        f'<div style="position: sticky; top: 0; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; z-index: 999; padding: 1rem 1.5rem; border-radius: 8px; margin-bottom: 1.5rem; box-shadow: 0 4px 6px rgba(0,0,0,0.1);"><b style="font-size: 1.1rem;">üìå Required Field Mapping Progress</b>{progress_html}</div>',
        unsafe_allow_html=True
    )

    # --- UX Tools (Collapsible Container) ---
    with st.expander("‚öôÔ∏è Tools & Actions", expanded=False):
        # Search field at the top of the container
        search_query = st.text_input("üîç Search Fields", placeholder="Type to filter fields... (Ctrl+F)", key="field_search")
        st.markdown("<br>", unsafe_allow_html=True)
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("**History**")
            # Initialize undo/redo
            initialize_undo_redo()
            final_mapping = st.session_state.get("final_mapping", {})
            
            # Initialize history with current state if empty
            if len(st.session_state.mapping_history) == 0:
                if final_mapping:
                    save_to_history(final_mapping)
                else:
                    # Save empty state as initial state
                    save_to_history({})
            
            # Check if undo/redo is possible
            can_undo = st.session_state.history_index > 0
            can_redo = st.session_state.history_index < len(st.session_state.mapping_history) - 1
            
            if st.button("‚Ü∂ Undo", key="undo_btn", use_container_width=True, disabled=not can_undo, help="Undo last mapping change (Ctrl+Z)"):
                undone = undo_mapping()
                if undone is not None:
                    import copy
                    st.session_state.final_mapping = copy.deepcopy(undone)
                    # Clear auto_mapping to force refresh
                    if "auto_mapping" in st.session_state:
                        del st.session_state.auto_mapping
                    st.rerun()
            
            if st.button("‚Ü∑ Redo", key="redo_btn", use_container_width=True, disabled=not can_redo, help="Redo last undone change (Ctrl+Y)"):
                redone = redo_mapping()
                if redone is not None:
                    import copy
                    st.session_state.final_mapping = copy.deepcopy(redone)
                    # Clear auto_mapping to force refresh
                    if "auto_mapping" in st.session_state:
                        del st.session_state.auto_mapping
                    st.rerun()
        
        with col2:
            st.markdown("**Bulk Actions**")
            ai_suggestions = st.session_state.get("auto_mapping", {})
            final_mapping = st.session_state.get("final_mapping", {})
            if st.button("‚úÖ Accept All AI (‚â•80%)", key="bulk_accept_ai", use_container_width=True):
                accepted = 0
                for field, info in ai_suggestions.items():
                    score = info.get("score", 0)
                    if score >= 80 and (field not in final_mapping or not final_mapping[field].get("value")):
                        final_mapping[field] = {"mode": "auto", "value": info["value"]}
                        accepted += 1
                if accepted > 0:
                    st.session_state.final_mapping = final_mapping
                    save_to_history(final_mapping)
                    st.success(f"Accepted {accepted} AI suggestions!")
                    st.rerun()
            if st.button("üîÑ Clear All", key="bulk_clear", use_container_width=True):
                if st.checkbox("Confirm: Clear all mappings?", key="confirm_clear"):
                    st.session_state.final_mapping = {}
                    save_to_history({})
                    st.success("All mappings cleared!")
                    st.rerun()
        
        with col3:
            st.markdown("**Utilities**")
            if st.button("üìã Copy Mapping", key="bulk_copy", use_container_width=True):
                import json
                mapping_str = json.dumps(final_mapping, indent=2)
                st.code(mapping_str, language="json")
                st.info("Right-click and copy the JSON above")

    # --- Main Mapping Section ---
    st.markdown("## Manual Field Mapping")
    # Gate heavy mapping updates behind a form submit to avoid recomputation on every rerun
    with st.form("mapping_form"):
        render_field_mapping_tab()
        apply_mappings = st.form_submit_button("Apply Mappings")
        if apply_mappings:
            st.session_state["mappings_ready"] = True
            # Save to history when form is submitted
            final_mapping = st.session_state.get("final_mapping", {})
            if final_mapping:
                save_to_history(final_mapping)

    st.divider()

    # --- AI Suggestions Section ---
    st.markdown("## AI Auto-Mapping Suggestions")

    # Compute AI suggestions only when needed
    if "auto_mapping" not in st.session_state and st.session_state.get("mappings_ready"):
        with st.spinner("Running AI mapping suggestions..."):
            st.session_state.auto_mapping = get_enhanced_automap(layout_df, claims_df)

    ai_suggestions = st.session_state.get("auto_mapping", {})
    auto_mapped_fields = st.session_state.get("auto_mapped_fields", [])

    if ai_suggestions:
        st.info("Fields with AI confidence ‚â• 80% have already been auto-mapped. You can override them manually below.")

        with st.expander("üîç View and Commit Additional Suggestions", expanded=False):
            selected_fields: List[str] = []

            for field, info in ai_suggestions.items():
                if field in auto_mapped_fields:
                    continue

                col1, col2, col3 = st.columns([3, 3, 1])
                with col1:
                    st.markdown(f"**{field}**")
                with col2:
                    st.code(info["value"], language="plaintext")
                    if "score" in info:
                        st.caption(f"Confidence: {info['score']}%")
                with col3:
                    selected = st.checkbox("Accept", key=f"ai_accept_{field}")
                    if selected:
                        selected_fields.append(field)

                if selected_fields and st.button("‚úÖ Commit Selected Suggestions"):
                    for field in selected_fields:
                        st.session_state.final_mapping[field] = {
                            "mode": "auto",
                            "value": ai_suggestions[field]["value"]
                        }

                    with st.spinner("Applying selected suggestions..."):
                        st.success(f"Committed {len(selected_fields)} suggestion(s).")
                        generate_all_outputs()

                        # --- Refresh transformed dataframe ---
                        claims_df = st.session_state.get("claims_df")
                        final_mapping = st.session_state.get("final_mapping", {})
                        if claims_df is not None and final_mapping:
                            st.session_state.transformed_df = transform_claims_data(claims_df, final_mapping)
                            st.session_state["transformed_ready"] = True

    else:
        st.info("No additional AI mapping suggestions available.")

with tab3:
    transformed_df = st.session_state.get("transformed_df")
    final_mapping = st.session_state.get("final_mapping", {})
    layout_df = st.session_state.get("layout_df")

    if transformed_df is None or not final_mapping:
        st.info("Please complete field mappings and preview transformed data first.")
        st.stop()

    # --- Auto-run validation (cached to avoid re-running on every rerun) ---
    # Create a hash of the data to detect changes
    data_hash = hashlib.md5(
        (str(final_mapping) + str(transformed_df.shape) + str(transformed_df.columns.tolist())).encode()
    ).hexdigest()
    
    # Check if we need to re-run validations
    cached_hash = st.session_state.get("validation_data_hash")
    validation_results = st.session_state.get("validation_results", [])
    
    if cached_hash != data_hash or not validation_results:
        with st.spinner("Running validation checks..."):
            # Get required fields from layout file, not from mapping
            if layout_df is not None:
                required_fields_df = get_required_fields(layout_df)
                required_fields = required_fields_df["Internal Field"].tolist() if isinstance(required_fields_df, pd.DataFrame) else []
            else:
                # Fallback: use all mapped fields as required
                required_fields = list(final_mapping.keys())
            
            # Get ALL mapped internal fields (both required and optional)
            all_mapped_internal_fields = [field for field in final_mapping.keys() if final_mapping[field].get("value")]
            
            # Get mapped field values (source column names) for reference
            mapped_fields = [mapping["value"] for mapping in final_mapping.values() if mapping.get("value")]
            
            # Run field-level validations (row-by-row checks)
            # Pass all mapped internal fields to ensure comprehensive validation
            field_level_results = run_validations(transformed_df, required_fields, all_mapped_internal_fields)
            
            # Run file-level validations (summary/aggregate checks)
            file_level_results = dynamic_run_validations(transformed_df, final_mapping)
            
            # Combine both types of validation results
            validation_results = field_level_results + file_level_results
            st.session_state.validation_results = validation_results
            st.session_state.validation_data_hash = data_hash

    # --- Validation Metrics Summary ---
    st.markdown("### Validation Summary")

    fails = [r for r in validation_results if r["status"] == "Fail"]
    warnings = [r for r in validation_results if r["status"] == "Warning"]
    passes = [r for r in validation_results if r["status"] == "Pass"]

    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric(label="Total Checks", value=len(validation_results))
    with col2:
        st.metric(label="‚úÖ Passes", value=len(passes))
    with col3:
        st.metric(label="‚ö†Ô∏è Warnings / ‚ùå Fails", value=len(warnings) + len(fails))

    st.divider()

    # --- Detailed Validation Table ---
    st.markdown("### Detailed Validation Results")
    if validation_results:
        validation_df = pd.DataFrame(validation_results)
        st.dataframe(validation_df, use_container_width=True)  # type: ignore[no-untyped-call]

        val_csv = validation_df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="üì• Download Validation Report",
            data=val_csv,
            file_name="validation_report.csv",
            mime="text/csv",
            on_click=lambda: _notify("‚úÖ Validation Report Ready!")
        )
    else:
        st.info("No validation results to display. Click 'Run Validation' to perform checks.")

    st.divider()

    # --- Final Verdict Block ---
    st.markdown("### Final Verdict")

    if not validation_results:
        st.info("No validations have been run yet.")
    else:
        if fails:
            st.markdown(
                """
                <div style='background-color:#fdecea; padding: 1rem; border-radius: 8px;'>
                <strong style='color: #b02a37;'>‚ùå File Status: Rejected ‚Äî Critical issues found.</strong>
                </div>
                """,
                unsafe_allow_html=True
            )
        elif warnings:
            st.markdown(
                """
                <div style='background-color:#fff3cd; padding: 1rem; border-radius: 8px;'>
                <strong style='color: #8a6d3b;'>‚ö†Ô∏è File Status: Warning ‚Äî Minor issues detected.</strong>
                </div>
                """,
                unsafe_allow_html=True
            )
        else:
            st.markdown(
                """
                <div style='background-color:#d4edda; padding: 1rem; border-radius: 8px;'>
                <strong style='color: #155724;'>‚úÖ File Status: Approved ‚Äî All checks passed.</strong>
                </div>
                """,
                unsafe_allow_html=True
            )

    # --- Rejection Reason Generator (only if Rejected) ---
    if fails:
        st.markdown("### Rejection Reason for Client Success Team")
        st.caption("Copy this paragraph to explain rejection to the client")

        layout_df = st.session_state.get("layout_df")
        final_mapping = st.session_state.get("final_mapping", {})

        # Extract required fields from layout (guard None and typing)
        if layout_df is None:
            required_fields: List[str] = []
        else:
            required_fields = layout_df[layout_df["Usage"].astype(str).str.lower() == "required"]["Internal Field"].tolist()  # type: ignore[no-untyped-call]

        # Recompute unmapped fields accurately
        unmapped_required_fields: List[str] = []
        for field in required_fields:
            mapping = final_mapping.get(field)
            if not mapping or not mapping.get("value") or str(mapping.get("value")).strip() == "":
                unmapped_required_fields.append(field)

        # --- Generate rejection reason ---
        if unmapped_required_fields:
            field_list = ", ".join(f"`{f}`" for f in unmapped_required_fields)
            reason_text = (
                "This file cannot be approved for automated processing because the following mandatory fields "
                f"required for Targeted Marketing setup are missing: {field_list}. "
                "Please ensure these fields are present and reupload the file."
            )
        else:
            reason_text = (
                "This file cannot be approved due to failed validation checks. "
                "Please review and correct the issues."
            )

        st.code(reason_text, language="markdown")

with tab4:
    st.markdown("## Final Outputs and Downloads")

    if "final_mapping" not in st.session_state or not st.session_state.final_mapping:
        st.info("Complete required field mappings to generate outputs.")
    else:
        final_mapping = st.session_state.get("final_mapping", {})
        layout_df = st.session_state.get("layout_df")
        claims_df = st.session_state.get("claims_df")
        anonymized_df = st.session_state.get("anonymized_df")
        mapping_table = st.session_state.get("mapping_table")
        transformed_df = st.session_state.get("transformed_df")

        if any(x is None for x in [anonymized_df, mapping_table, transformed_df]):
            st.warning("Outputs not fully generated yet. Please complete mapping and preview steps.")
        else:
            # --- Anonymized Claims File Section ---
            with st.expander("Anonymized Claims Preview", expanded=True):
                anonymized_df = st.session_state.get("anonymized_df")
                st.dataframe(anonymized_df.head(), use_container_width=True)  # type: ignore[no-untyped-call]

                st.markdown("**Customize Anonymized File Output**")
                col1, col2, col3 = st.columns(3)

                with col1:
                    file_name_input = st.text_input("File Name (without extension)", value="anonymized_claims")
                with col2:
                    file_type = st.selectbox("File Type", options=[".csv", ".txt", ".xlsx"], index=0)
                with col3:
                    delimiter = st.selectbox("Delimiter", options=["Comma", "Tab", "Pipe"], index=0)

                delim_char = {
                    "Comma": ",",
                    "Tab": "\t",
                    "Pipe": "|"
                }[delimiter]

                if file_type == ".xlsx":
                    output = io.BytesIO()
                    with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
                        anonymized_df.to_excel(writer, index=False)  # type: ignore[no-untyped-call]
                    anonymized_data = output.getvalue()
                    mime = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                else:
                    anonymized_data = anonymized_df.to_csv(index=False, sep=delim_char).encode('utf-8')  # type: ignore[no-untyped-call]
                    mime = "text/plain" if file_type == ".txt" else "text/csv"

                full_filename = f"{file_name_input.strip() or 'anonymized_claims'}{file_type}"

                # ‚úÖ Save to session state for ZIP bundling
                st.session_state.anonymized_data = anonymized_data
                st.session_state.anonymized_filename = full_filename

                col1, col2 = st.columns(2)
                with col1:
                    st.download_button(
                        label=f"Download {file_type.upper()}",
                        data=anonymized_data,
                        file_name=full_filename,
                        mime=mime,
                        key="download_anon",
                        on_click=lambda: _notify("‚úÖ Anonymized File Ready!")
                    )
                with col2:
                    if st.button("Regenerate Anonymized File"):
                        with st.spinner("Regenerating anonymized data..."):
                            claims_df_local = st.session_state.get("claims_df")
                            final_mapping_local = st.session_state.get("final_mapping", {})
                            if claims_df_local is not None:
                                st.session_state.anonymized_df = anonymize_claims_data(
                                    claims_df_local,
                                    final_mapping_local
                                )
                            _notify("‚úÖ Anonymized file regenerated!")

            # --- Field Mapping Table Section ---
            with st.expander("Field Mapping Table Preview", expanded=True):
                mapping_table = st.session_state.get("mapping_table")
                st.dataframe(mapping_table, use_container_width=True)  # type: ignore[no-untyped-call]
                mapping_csv = mapping_table.to_csv(index=False).encode('utf-8')  # type: ignore[no-untyped-call]
                col1, col2 = st.columns(2)
                with col1:
                    st.download_button(
                        label="Download Mapping Table",
                        data=mapping_csv,
                        file_name="field_mapping_table.csv",
                        mime="text/csv",
                        key="download_mapping",
                        on_click=lambda: _notify("‚úÖ Field Mapping Table Ready!")
                    )
                with col2:
                    if st.button("Regenerate Mapping Table"):
                        with st.spinner("Regenerating mapping table..."):
                            layout_df_local = st.session_state.get("layout_df")
                            claims_df_local = st.session_state.get("claims_df")
                            final_mapping_local = st.session_state.get("final_mapping", {})
                            if layout_df_local is not None and claims_df_local is not None:
                                st.session_state.mapping_table = generate_mapping_table(
                                    layout_df_local,
                                    final_mapping_local,
                                    claims_df_local
                                )
                            _notify("‚úÖ Mapping table regenerated!")

            # --- Optional Attachments Section ---
            st.markdown("### Optional Attachments to Include in ZIP")
            uploaded_attachments = st.file_uploader(
                "Attach any additional files (e.g., header file, original claims, notes)",
                accept_multiple_files=True,
                key="zip_attachments"
            )

            # --- Custom Notes Section ---
            st.markdown("### Add Custom Notes (Optional)")
            notes_text = st.text_area("Include any notes or instructions to be added in the README file:", key="readme_notes")

            # --- Download All as ZIP ---
            st.divider()
            st.markdown("### Download All Outputs as ZIP")

            readme_text = notes_text.strip() if notes_text else "No additional notes provided."
            anon_file_name = st.session_state.get("anonymized_filename", "anonymized_claims.csv")
            anonymized_data = st.session_state.get("anonymized_data", b"")

            buffer = io.BytesIO()
            with zipfile.ZipFile(buffer, "w") as zip_file:
                zip_file.writestr(anon_file_name, anonymized_data)
                zip_file.writestr("field_mapping_table.csv", mapping_csv)
                zip_file.writestr("readme.txt", readme_text)
                for attachment in uploaded_attachments or []:  # type: ignore[var-annotated]
                    att_name: Any = attachment.name  # type: ignore[attr-defined]
                    att_bytes: Any = attachment.getvalue()  # type: ignore[call-arg]
                    zip_file.writestr(att_name, att_bytes)  # type: ignore[arg-type]

            buffer.seek(0)

            col1, col2 = st.columns(2)
            with col1:
                st.download_button(
                    label="Download All Files (ZIP)",
                    data=buffer,
                    file_name="all_outputs.zip",
                    mime="application/zip",
                    key="download_zip"
                )
            with col2:
                if st.button("Regenerate All Outputs"):
                    with st.spinner("Regenerating all outputs..."):
                        if claims_df is not None:
                            st.session_state.anonymized_df = anonymize_claims_data(claims_df, final_mapping)
                        if layout_df is not None and claims_df is not None:
                            st.session_state.mapping_table = generate_mapping_table(layout_df, final_mapping, claims_df)
                        _notify("All outputs regenerated.")
